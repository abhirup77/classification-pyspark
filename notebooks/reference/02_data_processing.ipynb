{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "\n",
    "This notebook demonstrates the data pipeline from raw tables to analytical datasets. At the end of this activity, train & test data sets are created from raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:param.panel_extension: bokeh extension not recognized and will be skipped.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error() {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (var i = 0; i < css_urls.length; i++) {\n",
       "      var url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    var skip = [];\n",
       "    if (window.requirejs) {\n",
       "      require([], function() {\n",
       "      })\n",
       "    }\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n",
       "      var element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error;\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "\tif (!js_urls.length) {\n",
       "      on_load()\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\", \"https://unpkg.com/@holoviz/panel@^0.10.1/dist/panel.min.js\"];\n",
       "  var css_urls = [\"https://unpkg.com/@holoviz/panel@0.10.1/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.10.1/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.10.1/dist/css/dataframe.css\", \"https://unpkg.com/@holoviz/panel@0.10.1/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.10.1/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.10.1/dist/css/widgets.css\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {} // ensure no trailing comma for IE\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    }\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.holoviews_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      require([], function() {\n      })\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) >= 0) { on_load(); continue; }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n\tif (!js_urls.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.2.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.2.3.min.js\", \"https://unpkg.com/@holoviz/panel@^0.10.1/dist/panel.min.js\"];\n  var css_urls = [\"https://unpkg.com/@holoviz/panel@0.10.1/dist/css/alerts.css\", \"https://unpkg.com/@holoviz/panel@0.10.1/dist/css/card.css\", \"https://unpkg.com/@holoviz/panel@0.10.1/dist/css/dataframe.css\", \"https://unpkg.com/@holoviz/panel@0.10.1/dist/css/json.css\", \"https://unpkg.com/@holoviz/panel@0.10.1/dist/css/markdown.css\", \"https://unpkg.com/@holoviz/panel@0.10.1/dist/css/widgets.css\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "if ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n",
       "  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n",
       "}\n",
       "\n",
       "\n",
       "    function JupyterCommManager() {\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n",
       "      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        comm_manager.register_target(comm_id, function(comm) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        });\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        });\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n",
       "          var messages = comm.messages[Symbol.asyncIterator]();\n",
       "          function processIteratorResult(result) {\n",
       "            var message = result.value;\n",
       "            console.log(message)\n",
       "            var content = {data: message.data, comm_id};\n",
       "            var buffers = []\n",
       "            for (var buffer of message.buffers || []) {\n",
       "              buffers.push(new DataView(buffer))\n",
       "            }\n",
       "            var metadata = message.metadata || {};\n",
       "            var msg = {content, buffers, metadata}\n",
       "            msg_handler(msg);\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "          return messages.next().then(processIteratorResult);\n",
       "        })\n",
       "      }\n",
       "    }\n",
       "\n",
       "    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n",
       "      if (comm_id in window.PyViz.comms) {\n",
       "        return window.PyViz.comms[comm_id];\n",
       "      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n",
       "        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n",
       "        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n",
       "        if (msg_handler) {\n",
       "          comm.on_msg(msg_handler);\n",
       "        }\n",
       "      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n",
       "        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n",
       "        comm.open();\n",
       "        if (msg_handler) {\n",
       "          comm.onMsg = msg_handler;\n",
       "        }\n",
       "      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n",
       "        var comm_promise = google.colab.kernel.comms.open(comm_id)\n",
       "        comm_promise.then((comm) => {\n",
       "          window.PyViz.comms[comm_id] = comm;\n",
       "          if (msg_handler) {\n",
       "            var messages = comm.messages[Symbol.asyncIterator]();\n",
       "            function processIteratorResult(result) {\n",
       "              var message = result.value;\n",
       "              var content = {data: message.data};\n",
       "              var metadata = message.metadata || {comm_id};\n",
       "              var msg = {content, metadata}\n",
       "              msg_handler(msg);\n",
       "              return messages.next().then(processIteratorResult);\n",
       "            }\n",
       "            return messages.next().then(processIteratorResult);\n",
       "          }\n",
       "        }) \n",
       "        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n",
       "          return comm_promise.then((comm) => {\n",
       "            comm.send(data, metadata, buffers, disposeOnDone);\n",
       "          });\n",
       "        };\n",
       "        var comm = {\n",
       "          send: sendClosure\n",
       "        };\n",
       "      }\n",
       "      window.PyViz.comms[comm_id] = comm;\n",
       "      return comm;\n",
       "    }\n",
       "    window.PyViz.comm_manager = new JupyterCommManager();\n",
       "    \n",
       "\n",
       "\n",
       "var JS_MIME_TYPE = 'application/javascript';\n",
       "var HTML_MIME_TYPE = 'text/html';\n",
       "var EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\n",
       "var CLASS_NAME = 'output';\n",
       "\n",
       "/**\n",
       " * Render data to the DOM node\n",
       " */\n",
       "function render(props, node) {\n",
       "  var div = document.createElement(\"div\");\n",
       "  var script = document.createElement(\"script\");\n",
       "  node.appendChild(div);\n",
       "  node.appendChild(script);\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when a new output is added\n",
       " */\n",
       "function handle_add_output(event, handle) {\n",
       "  var output_area = handle.output_area;\n",
       "  var output = handle.output;\n",
       "  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "    return\n",
       "  }\n",
       "  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "  if (id !== undefined) {\n",
       "    var nchildren = toinsert.length;\n",
       "    var html_node = toinsert[nchildren-1].children[0];\n",
       "    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var scripts = [];\n",
       "    var nodelist = html_node.querySelectorAll(\"script\");\n",
       "    for (var i in nodelist) {\n",
       "      if (nodelist.hasOwnProperty(i)) {\n",
       "        scripts.push(nodelist[i])\n",
       "      }\n",
       "    }\n",
       "\n",
       "    scripts.forEach( function (oldScript) {\n",
       "      var newScript = document.createElement(\"script\");\n",
       "      var attrs = [];\n",
       "      var nodemap = oldScript.attributes;\n",
       "      for (var j in nodemap) {\n",
       "        if (nodemap.hasOwnProperty(j)) {\n",
       "          attrs.push(nodemap[j])\n",
       "        }\n",
       "      }\n",
       "      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n",
       "      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n",
       "      oldScript.parentNode.replaceChild(newScript, oldScript);\n",
       "    });\n",
       "    if (JS_MIME_TYPE in output.data) {\n",
       "      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n",
       "    }\n",
       "    output_area._hv_plot_id = id;\n",
       "    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n",
       "      window.PyViz.plot_index[id] = Bokeh.index[id];\n",
       "    } else {\n",
       "      window.PyViz.plot_index[id] = null;\n",
       "    }\n",
       "  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "    var bk_div = document.createElement(\"div\");\n",
       "    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "    var script_attrs = bk_div.children[0].attributes;\n",
       "    for (var i = 0; i < script_attrs.length; i++) {\n",
       "      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "    }\n",
       "    // store reference to server id on output_area\n",
       "    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle when an output is cleared or removed\n",
       " */\n",
       "function handle_clear_output(event, handle) {\n",
       "  var id = handle.cell.output_area._hv_plot_id;\n",
       "  var server_id = handle.cell.output_area._bokeh_server_id;\n",
       "  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n",
       "  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n",
       "  if (server_id !== null) {\n",
       "    comm.send({event_type: 'server_delete', 'id': server_id});\n",
       "    return;\n",
       "  } else if (comm !== null) {\n",
       "    comm.send({event_type: 'delete', 'id': id});\n",
       "  }\n",
       "  delete PyViz.plot_index[id];\n",
       "  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n",
       "    var doc = window.Bokeh.index[id].model.document\n",
       "    doc.clear();\n",
       "    const i = window.Bokeh.documents.indexOf(doc);\n",
       "    if (i > -1) {\n",
       "      window.Bokeh.documents.splice(i, 1);\n",
       "    }\n",
       "  }\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle kernel restart event\n",
       " */\n",
       "function handle_kernel_cleanup(event, handle) {\n",
       "  delete PyViz.comms[\"hv-extension-comm\"];\n",
       "  window.PyViz.plot_index = {}\n",
       "}\n",
       "\n",
       "/**\n",
       " * Handle update_display_data messages\n",
       " */\n",
       "function handle_update_output(event, handle) {\n",
       "  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n",
       "  handle_add_output(event, handle)\n",
       "}\n",
       "\n",
       "function register_renderer(events, OutputArea) {\n",
       "  function append_mime(data, metadata, element) {\n",
       "    // create a DOM node to render to\n",
       "    var toinsert = this.create_output_subarea(\n",
       "    metadata,\n",
       "    CLASS_NAME,\n",
       "    EXEC_MIME_TYPE\n",
       "    );\n",
       "    this.keyboard_manager.register_events(toinsert);\n",
       "    // Render to node\n",
       "    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "    render(props, toinsert[0]);\n",
       "    element.append(toinsert);\n",
       "    return toinsert\n",
       "  }\n",
       "\n",
       "  events.on('output_added.OutputArea', handle_add_output);\n",
       "  events.on('output_updated.OutputArea', handle_update_output);\n",
       "  events.on('clear_output.CodeCell', handle_clear_output);\n",
       "  events.on('delete.Cell', handle_clear_output);\n",
       "  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n",
       "\n",
       "  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "    safe: true,\n",
       "    index: 0\n",
       "  });\n",
       "}\n",
       "\n",
       "if (window.Jupyter !== undefined) {\n",
       "  try {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  } catch(err) {\n",
       "  }\n",
       "}\n"
      ],
      "application/vnd.holoviews_load.v0+json": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Standard Library Imports\n",
    "import sys\n",
    "import time\n",
    "import os.path as op\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Third Party Imports\n",
    "import yaml\n",
    "import hvplot\n",
    "import panel as pn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "from pyspark_dist_explore import (\n",
    "    Histogram,\n",
    "    hist,\n",
    "    distplot,\n",
    "    pandas_histogram\n",
    ")\n",
    "from IPython.display import (\n",
    "    display,\n",
    "    display_html\n",
    ")\n",
    "\n",
    "# Spark Imports\n",
    "from pyspark.sql import (\n",
    "    types as DT,\n",
    "    functions as F,\n",
    "    Window\n",
    ")\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import (\n",
    "    ParamGridBuilder,\n",
    "    CrossValidator,\n",
    "    CrossValidatorModel\n",
    ")\n",
    "from pyspark.ml.feature import (\n",
    "    VectorAssembler,\n",
    "    StandardScaler,\n",
    "    StringIndexer,\n",
    "    OneHotEncoderEstimator,\n",
    "    Imputer\n",
    ")\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "# Project Imports\n",
    "from ta_lib.pyspark import (\n",
    "    dp,\n",
    "    features,\n",
    "    model_gen,\n",
    "    model_eval,\n",
    "    utils,\n",
    "    eda,\n",
    "    context\n",
    ")\n",
    "\n",
    "# options\n",
    "random_seed = 0\n",
    "pn.extension('bokeh')\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "\n",
    "`config.yml` is used to store all the parameters required for the template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all': {'core': 'default',\n",
       "  'log_catalog': 'production',\n",
       "  'data_catalog': 'local',\n",
       "  'job_catalog': 'local'},\n",
       " 'spark': {'spark.executer.cores': 4, 'spark.cores.max': 4}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_path = op.join(os.getcwd(),'conf', 'config.yml')\n",
    "with open(config_path, 'r') as fp:\n",
    "    config = yaml.load(fp)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reference_date': datetime.date(2019, 4, 26),\n",
       " 'num_days_prediction': 90,\n",
       " 'raw': {'filesystem': 'dbfs',\n",
       "  'base_path': '/FileStore/tables/vacation_partitioned/',\n",
       "  'call_data_path': 'dial_summary.parquet',\n",
       "  'last_activity_data_path': 'customer_activity.parquet',\n",
       "  'booking_data_path': 'class_labels.parquet',\n",
       "  'consumer_data_path': 'customer.parquet',\n",
       "  'web_data_path': 'itr_data_*.parquet'},\n",
       " 'clean': {'filesystem': 'dbfs',\n",
       "  'base_path': '/FileStore/tables/vacation_clean/',\n",
       "  'call_data_path': 'dial_summary.parquet',\n",
       "  'last_activity_data_path': 'customer_activity.parquet',\n",
       "  'booking_data_path': 'class_labels.parquet',\n",
       "  'consumer_data_path': 'customer.parquet',\n",
       "  'web_data_path': 'itr_data_*.parquet'},\n",
       " 'processed': {'filesystem': 'dbfs',\n",
       "  'base_path': '/FileStore/tables/spark_warehouse/',\n",
       "  'train': 'train.parquet',\n",
       "  'test': 'test.parquet',\n",
       "  'preds': 'predictions.parquet'},\n",
       " 'spark': {'spark.executer.cores': 4, 'spark.cores.max': 4}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_config_path = op.join(os.getcwd(),'conf/data_catalog', 'remote.yml')\n",
    "with open(data_config_path, 'r') as fp:\n",
    "    data_config = yaml.load(fp)\n",
    "data_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spark session\n",
    "\n",
    "`talib.pyspark.context` module is leveraged to build the sparksession so as to consider the spark session related params in the config file while building the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "session = context.CustomSparkSession(config)\n",
    "session.CreateSparkSession()\n",
    "spark = session.spark\n",
    "sc = session.sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.98 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_call_data = dp.read_data(\n",
    "    spark=spark,\n",
    "    paths=[data_config['raw']['base_path'] + data_config['raw']['call_data_path']],\n",
    "    fs=data_config['raw']['filesystem'],\n",
    "    fmt=\"parquet\",\n",
    "    header=\"true\",\n",
    "    inferschema=\"true\"\n",
    ")\n",
    "df_last_activity_data = dp.read_data(\n",
    "    spark=spark,\n",
    "    paths=[data_config['raw']['base_path'] + data_config['raw']['last_activity_data_path']],\n",
    "    fs=data_config['raw']['filesystem'],\n",
    "    fmt=\"parquet\",\n",
    "    header=\"true\",\n",
    "    inferschema=\"true\"\n",
    ")\n",
    "df_booking_data = dp.read_data(\n",
    "    spark=spark,\n",
    "    paths=[data_config['raw']['base_path'] + data_config['raw']['booking_data_path']],\n",
    "    fs=data_config['raw']['filesystem'],\n",
    "    fmt=\"parquet\",\n",
    "    header=\"true\",\n",
    "    inferschema=\"true\"\n",
    ")\n",
    "df_consumer_data = dp.read_data(\n",
    "    spark=spark,\n",
    "    paths=[data_config['raw']['base_path'] + data_config['raw']['consumer_data_path']],\n",
    "    fs=data_config['raw']['filesystem'],\n",
    "    fmt=\"parquet\",\n",
    "    header=\"true\",\n",
    "    inferschema=\"true\"\n",
    ")\n",
    "df_web_data = dp.read_data(\n",
    "    spark=spark,\n",
    "    paths=[data_config['raw']['base_path'] + data_config['raw']['web_data_path']],\n",
    "    fs=data_config['raw']['filesystem'],\n",
    "    fmt=\"parquet\",\n",
    "    header=\"true\",\n",
    "    inferschema=\"true\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "The focus here is to create a cleaned dataset that is appropriate for solving the DS problem at hand from the raw data.\n",
    "\n",
    "**Do's**\n",
    "\n",
    "* Clean dataframe column names\n",
    "* Ensure dtypes are set properly\n",
    "* Join with other tables etc to create features\n",
    "* Transform, if appropriate, datetime like columns to generate additional features (weekday etc)\n",
    "* Discard cols that are not useful for training the model (IDs, constant cols, duplicate cols etc) additional features generated from existing columns\n",
    "\n",
    "**Dont's**\n",
    "\n",
    "* Handle missing values or outliers here. Mark them and leave them for processing downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Dates\n",
    "\n",
    "`current_date` - date of reference for consideration for historical data.<br>\n",
    "`pred_period_start` `and pred_period_end` - date range for building the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference date: 2019-04-26 00:00:00 \t Prediction start date: 2019-04-27 00:00:00 \t Prediction end date: 2019-07-25 00:00:00 \n"
     ]
    }
   ],
   "source": [
    "reference_date = pd.to_datetime(data_config['reference_date'])\n",
    "pred_period_start = reference_date + pd.Timedelta(days=1)\n",
    "pred_period_end = reference_date + pd.Timedelta(days=data_config['num_days_prediction'])\n",
    "print(f\"Reference date: {reference_date} \\t Prediction start date: {pred_period_start} \\t Prediction end date: {pred_period_end} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Filtering\n",
    "\n",
    "Choose `reference_date` (present in the config) post which we will predict the customer had made the booking in next 3 months.<br>\n",
    "Filter all Feature dataset on the `reference_date`.<br>\n",
    "Filter target dataset on `Prediction period start` and `Prediction period end`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter call_data dataframe\n",
    "df_call_data = df_call_data.withColumn(\n",
    "    'call_date', \n",
    "    F.to_date(F.unix_timestamp(F.col(\"call_date\"), \"ddMMMyyyy:HH:mm:ss\").cast(\"timestamp\"))\n",
    ")\n",
    "df_call_data = df_call_data.filter(F.col('call_date')<=reference_date)\n",
    "\n",
    "# Filter last_activity dataframe\n",
    "df_last_activity_data = df_last_activity_data.dropDuplicates(['customer_id','actvty_date','actvty_type','actvty_prod'])\n",
    "last_activity_date_cols = ['load_date','actvty_date']\n",
    "for col in last_activity_date_cols:\n",
    "    df_last_activity_data = df_last_activity_data.withColumn(\n",
    "        col, \n",
    "        F.to_date(F.unix_timestamp(F.col(col), \"ddMMMyyyy:HH:mm:ss\").cast(\"timestamp\"))\n",
    "    )\n",
    "df_last_activity_data = df_last_activity_data.filter(F.col('actvty_date')<=reference_date)\n",
    "\n",
    "# Filter web_data dataframe\n",
    "df_web_data = df_web_data.dropDuplicates(['customer_id','visit_date','device_type_name','visit_type'])\n",
    "df_web_data = df_web_data.withColumn(\n",
    "    'visit_date', \n",
    "    F.to_date(F.unix_timestamp(F.col(\"visit_date\"), \"ddMMMyyyy:HH:mm:ss\").cast(\"timestamp\"))\n",
    ")\n",
    "df_web_data = df_web_data.filter(F.col('visit_date')<=reference_date)\n",
    "\n",
    "# Filter consumer_data dataframe\n",
    "drop_lst = ['click_pct',\n",
    "           'open_pct',\n",
    "           'max_event_date',\n",
    "           'booked_flag']\n",
    "df_consumer_data = df_consumer_data.drop(*drop_lst)\n",
    "df_consumer_data = df_consumer_data.withColumn(\n",
    "    'cel_first_cruise_date', \n",
    "    F.to_date(F.unix_timestamp(F.col(\"cel_first_cruise_date\"), \"ddMMMyyyy:HH:mm:ss\").cast(\"timestamp\"))\n",
    ")\n",
    "df_consumer_data = df_consumer_data.filter(F.col('cel_first_cruise_date')<=reference_date)\n",
    "\n",
    "# Filter booking_data dataframe\n",
    "df_booking_data = df_booking_data.withColumn(\n",
    "    'booking_create_date',\n",
    "    F.to_date(F.unix_timestamp(F.col(\"booking_create_date\"), \"ddMMMyyyy:HH:mm:ss\").cast(\"timestamp\"))\n",
    ")\n",
    "df_booking_data = df_booking_data.filter((F.col('booking_create_date') >= pred_period_start) \\\n",
    "                                        & (F.col('booking_create_date') <= pred_period_end))\n",
    "df_booking_data = df_booking_data.select('customer_id')\\\n",
    "                                .dropDuplicates()\\\n",
    "                                .withColumn('target_var',F.lit(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Feature Generation\n",
    "\n",
    "Find number of common consumers across all feature tables.<br>\n",
    "Binary column is built based on accurance of a booking for a consumer in the pred period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of common consumers in all feature tables\t: 106530\n",
      "Booked size\t: 13033\n",
      "Non booked size\t: 93497\n",
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_common_consumer = df_consumer_data.select('customer_id')\\\n",
    "                                    .join(df_call_data.select(\"customer_id\"), on ='customer_id',how='inner')\\\n",
    "                                    .join(df_web_data.select(\"customer_id\"), on='customer_id', how='inner')\\\n",
    "                                    .join(df_last_activity_data.select(\"customer_id\"), on='customer_id', how='inner')\\\n",
    "                                    .dropDuplicates()\n",
    "print(f\"Total number of common consumers in all feature tables\\t: {df_common_consumer.count()}\")\n",
    "\n",
    "# Map with target labels\n",
    "df_common_consumer_booking = df_common_consumer.join(df_booking_data, on='customer_id', how='left')\\\n",
    "                                            .dropDuplicates()\\\n",
    "                                            .fillna(0, subset=['target_var'])\n",
    "\n",
    "\n",
    "booked_size = df_common_consumer_booking.filter(F.col('target_var')==1).count()\n",
    "non_booked_size = df_common_consumer_booking.filter(F.col('target_var')==0).count()\n",
    "\n",
    "print(f\"Booked size\\t: {booked_size}\")\n",
    "print(f\"Non booked size\\t: {non_booked_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independant Features Generation\n",
    "\n",
    "last_call_date_diff - number of days since the consumer made the last call <br>\n",
    "last_act_date_diff - number of days since the consumer was last active <br>\n",
    "last_web_date_diff - number of days since the consumer was last web activity <br>\n",
    "total_sec_spent - total time spent in seconds. <br>\n",
    "total_page_view_count - total page view count. <br>\n",
    "age  <br>\n",
    "gender_code <br>\n",
    "state_code <br>\n",
    "rci_qualify_cruise_qty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# number of days since the consumer made the last call\n",
    "df_call_data = df_call_data.withColumn(\"last_call_date_diff\", \n",
    "                      F.datediff(F.to_date(F.lit(reference_date.strftime('%Y-%m-%d'))),\n",
    "                       F.col(\"call_date\")))\n",
    "\n",
    "df_call_data = df_call_data.groupby('customer_id').agg(F.min('last_call_date_diff').alias('last_call_date_diff'))\n",
    "\n",
    "df_call_data = df_call_data.where(F.col('customer_id').isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# number of days since the consumer was last active\n",
    "df_last_activity_data = df_last_activity_data.withColumn(\"last_act_date_diff\", \n",
    "                      F.datediff(F.to_date(F.lit(reference_date.strftime('%Y-%m-%d'))),\n",
    "                       F.col(\"actvty_date\")))\n",
    "\n",
    "df_last_activity_data = df_last_activity_data.groupby('customer_id').agg(F.min('last_act_date_diff').alias('last_act_date_diff'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 31.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# number of days since the consumer was last web activity, total time spent in seconds, total page view count\n",
    "df_web_data = df_web_data.withColumn(\"last_web_date_diff\", \n",
    "                      F.datediff(F.to_date(F.lit(reference_date.strftime('%Y-%m-%d'))),\n",
    "                       F.col(\"visit_date\")))\n",
    "\n",
    "df_web_data = df_web_data.groupby('customer_id').agg(F.min('last_web_date_diff').alias('last_web_date_diff'),\n",
    "                                                     F.sum('sec_time_spent_on_nbr').alias('total_sec_spent'),\n",
    "                                                     F.sum('page_view_count').alias('total_page_view_count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows:\t106530\n",
      "Total number of columns:\t11\n",
      "Wall time: 39.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Filtering the columns from the primary dataframe where all other feature tables would be merged\n",
    "df_consumer_data = df_consumer_data.select('customer_id','age','gender_code','state_code','rci_qualify_cruise_qty')\n",
    "\n",
    "df_consumer_data = df_consumer_data.join(df_call_data, on='customer_id', how='left')\\\n",
    "                        .join(df_last_activity_data, on='customer_id', how='left')\\\n",
    "                        .join(df_web_data, on='customer_id', how='left')\n",
    "\n",
    "df_consumer_data = df_consumer_data.join(df_common_consumer_booking, on='customer_id', how='inner')\n",
    "\n",
    "print(f\"Total number of rows:\\t{df_consumer_data.count()}\")\n",
    "print(f\"Total number of columns:\\t{len(df_consumer_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns type identification in the Final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3 ms\n"
     ]
    },
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='1001'>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"7a6996f4-412d-45d2-a8d6-6362fcc0eb9b\" data-root-id=\"1001\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  function embed_document(root) {\n",
       "    var docs_json = {\"7ceb9df7-ad20-4dac-959e-e797f98acf27\":{\"roots\":{\"references\":[{\"attributes\":{\"reload\":false},\"id\":\"1011\",\"type\":\"panel.models.location.Location\"},{\"attributes\":{\"child\":{\"id\":\"1006\"},\"name\":\"date_cols\",\"title\":\"date_cols\"},\"id\":\"1007\",\"type\":\"Panel\"},{\"attributes\":{\"child\":{\"id\":\"1002\"},\"name\":\"nemerical\",\"title\":\"nemerical\"},\"id\":\"1003\",\"type\":\"Panel\"},{\"attributes\":{\"child\":{\"id\":\"1004\"},\"name\":\"cat_cols\",\"title\":\"cat_cols\"},\"id\":\"1005\",\"type\":\"Panel\"},{\"attributes\":{\"margin\":[5,20,5,5],\"name\":\"cat_cols\",\"text\":\"[\\\"gender_code\\\", \\\"state_code\\\"]\"},\"id\":\"1004\",\"type\":\"panel.models.markup.JSON\"},{\"attributes\":{\"margin\":[5,20,5,5],\"name\":\"date_cols\",\"text\":\"{}\"},\"id\":\"1006\",\"type\":\"panel.models.markup.JSON\"},{\"attributes\":{\"margin\":[5,20,5,5],\"name\":\"bool_cols\",\"text\":\"{}\"},\"id\":\"1008\",\"type\":\"panel.models.markup.JSON\"},{\"attributes\":{\"client_comm_id\":\"748f2a6a3eb54b10b220e08ebfd7abdd\",\"comm_id\":\"8db9245358854c6ca93585e1f3c4438a\",\"plot_id\":\"1001\"},\"id\":\"1010\",\"type\":\"panel.models.comm_manager.CommManager\"},{\"attributes\":{\"margin\":[5,20,5,5],\"name\":\"nemerical\",\"text\":\"[\\\"customer_id\\\", \\\"age\\\", \\\"rci_qualify_cruise_qty\\\", \\\"last_call_date_diff\\\", \\\"last_act_date_diff\\\", \\\"last_web_date_diff\\\", \\\"total_sec_spent\\\", \\\"total_page_view_count\\\", \\\"target_var\\\"]\"},\"id\":\"1002\",\"type\":\"panel.models.markup.JSON\"},{\"attributes\":{\"child\":{\"id\":\"1008\"},\"name\":\"bool_cols\",\"title\":\"bool_cols\"},\"id\":\"1009\",\"type\":\"Panel\"},{\"attributes\":{\"margin\":[0,0,0,0],\"tabs\":[{\"id\":\"1003\"},{\"id\":\"1005\"},{\"id\":\"1007\"},{\"id\":\"1009\"}]},\"id\":\"1001\",\"type\":\"Tabs\"}],\"root_ids\":[\"1001\",\"1010\",\"1011\"]},\"title\":\"Bokeh Application\",\"version\":\"2.2.3\"}};\n",
       "    var render_items = [{\"docid\":\"7ceb9df7-ad20-4dac-959e-e797f98acf27\",\"root_ids\":[\"1001\"],\"roots\":{\"1001\":\"7a6996f4-412d-45d2-a8d6-6362fcc0eb9b\"}}];\n",
       "    root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "  }\n",
       "  if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined && root.Bokeh.Panel !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);</script>"
      ],
      "text/plain": [
       "Tabs\n",
       "    [0] JSON(list, name='nemerical')\n",
       "    [1] JSON(list, name='cat_cols')\n",
       "    [2] JSON(list, name='date_cols')\n",
       "    [3] JSON(list, name='bool_cols')"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "1001"
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "types = {\n",
    "    'nemerical': dp.list_numerical_columns,\n",
    "    'cat_cols': dp.list_categorical_columns,\n",
    "    'date_cols': dp.list_datelike_columns,\n",
    "    'bool_cols': dp.list_boolean_columns\n",
    "}\n",
    "utils.display_as_tabs([(k,v(df_consumer_data)) for k,v in types.items()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values Handling \n",
    "\n",
    "Handling them pre train-test-split as the features here are historic and can be imputed by considering the entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44.8 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>rci_qualify_cruise_qty</th>\n",
       "      <th>last_call_date_diff</th>\n",
       "      <th>last_act_date_diff</th>\n",
       "      <th>last_web_date_diff</th>\n",
       "      <th>total_sec_spent</th>\n",
       "      <th>total_page_view_count</th>\n",
       "      <th>target_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14313</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  age  gender_code  state_code  rci_qualify_cruise_qty  \\\n",
       "0            0    0            0       14313                       0   \n",
       "\n",
       "   last_call_date_diff  last_act_date_diff  last_web_date_diff  \\\n",
       "0                    0                   0                   0   \n",
       "\n",
       "   total_sec_spent  total_page_view_count  target_var  \n",
       "0                0                      0           0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dp.identify_missing_values(df_consumer_data).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the imputation is done using the below methods:\n",
    "1. Mean for numerical cols\n",
    "2. Mode for categorical cols and boolean cols\n",
    "\n",
    "We can explicitly define the method for imputation for each of the columns in the `rules` dictionary. \n",
    "<br>Method consists of :`[\"mean\", \"median\", \"mode\", \"constant\"]`\n",
    "\n",
    "For `Method=\"constant\"` , we can choose the impute value in the `impute_val` attribute. By default, `\"0\"` for numerical columns, and `\"NA\"` for all other column types\n",
    "\n",
    "\n",
    "\n",
    "    imputer = dp.Imputer(cols=[\"rci_qualify_cruise_qty\"],rules={\"rci_qualify_cruise_qty\":{\"method\":\"constant\",\"impute_val\":\"0\"}})\n",
    "    imputer.fit(df_consumer_data)\n",
    "    imputed_data = imputer.transform(df_consumer_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation cols and values:\t{'state_code': 'FL'}\n",
      "Wall time: 2min 5s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>age</th>\n",
       "      <th>gender_code</th>\n",
       "      <th>state_code</th>\n",
       "      <th>rci_qualify_cruise_qty</th>\n",
       "      <th>last_call_date_diff</th>\n",
       "      <th>last_act_date_diff</th>\n",
       "      <th>last_web_date_diff</th>\n",
       "      <th>total_sec_spent</th>\n",
       "      <th>total_page_view_count</th>\n",
       "      <th>target_var</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  age  gender_code  state_code  rci_qualify_cruise_qty  \\\n",
       "0            0    0            0           0                       0   \n",
       "\n",
       "   last_call_date_diff  last_act_date_diff  last_web_date_diff  \\\n",
       "0                    0                   0                   0   \n",
       "\n",
       "   total_sec_spent  total_page_view_count  target_var  \n",
       "0                0                      0           0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "imputer = dp.Imputer(cols=[])\n",
    "imputer.fit(df_consumer_data)\n",
    "imputed_data = imputer.transform(df_consumer_data)\n",
    "dp.identify_missing_values(imputed_data).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split of the Model Data\n",
    "\n",
    "We split the data into train, test (optionally, also a validation dataset).\n",
    "In this example, we are stratifying the data by target variable classes and split the data with 70% in train and remaining in test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90647 15883\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df, test_df = dp.test_train_split(\n",
    "    spark,\n",
    "    data=imputed_data,\n",
    "    target_col=\"target_var\",\n",
    "    train_prop=0.7,\n",
    "    random_seed=random_seed,\n",
    "    stratify=True,\n",
    "    target_type=\"categorical\"\n",
    ")\n",
    "print(train_df.count(), test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_data_path = data_config['processed']['filesystem'] + \":\" + data_config['processed']['base_path'] + data_config['processed']['train']\n",
    "test_data_path = data_config['processed']['filesystem'] + \":\" + data_config['processed']['base_path'] + data_config['processed']['test']\n",
    "\n",
    "train_df.write.mode(\"overwrite\").parquet(train_data_path)\n",
    "test_df.write.mode(\"overwrite\").parquet(test_data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
